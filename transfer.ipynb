{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8565512,"sourceType":"datasetVersion","datasetId":5120732},{"sourceId":652743,"sourceType":"modelInstanceVersion","modelInstanceId":493035,"modelId":508464},{"sourceId":653991,"sourceType":"modelInstanceVersion","modelInstanceId":494100,"modelId":508464}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchio\n!pip install focal_loss_torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T08:47:31.242372Z","iopub.execute_input":"2025-11-20T08:47:31.242954Z","iopub.status.idle":"2025-11-20T08:48:51.670010Z","shell.execute_reply.started":"2025-11-20T08:47:31.242928Z","shell.execute_reply":"2025-11-20T08:48:51.669164Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Collecting torchio\n  Downloading torchio-0.21.0-py3-none-any.whl.metadata (52 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: deprecated>=1.2 in /usr/local/lib/python3.11/dist-packages (from torchio) (1.3.1)\nRequirement already satisfied: einops>=0.3 in /usr/local/lib/python3.11/dist-packages (from torchio) (0.8.1)\nRequirement already satisfied: humanize>=0.1 in /usr/local/lib/python3.11/dist-packages (from torchio) (4.12.3)\nRequirement already satisfied: nibabel>=3 in /usr/local/lib/python3.11/dist-packages (from torchio) (5.3.2)\nRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from torchio) (1.26.4)\nRequirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from torchio) (25.0)\nRequirement already satisfied: rich>=10 in /usr/local/lib/python3.11/dist-packages (from torchio) (14.2.0)\nRequirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.11/dist-packages (from torchio) (1.15.3)\nRequirement already satisfied: simpleitk!=2.0.*,!=2.1.1.1,>=1.3 in /usr/local/lib/python3.11/dist-packages (from torchio) (2.5.2)\nRequirement already satisfied: torch>=1.9 in /usr/local/lib/python3.11/dist-packages (from torchio) (2.6.0+cu124)\nRequirement already satisfied: tqdm>=4.40 in /usr/local/lib/python3.11/dist-packages (from torchio) (4.67.1)\nRequirement already satisfied: typer>=0.1 in /usr/local/lib/python3.11/dist-packages (from torchio) (0.16.0)\nRequirement already satisfied: wrapt<3,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2->torchio) (1.17.2)\nRequirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel>=3->torchio) (6.5.2)\nRequirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.11/dist-packages (from nibabel>=3->torchio) (4.15.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->torchio) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->torchio) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->torchio) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->torchio) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->torchio) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->torchio) (2.4.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10->torchio) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10->torchio) (2.19.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->torchio) (3.20.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->torchio) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->torchio) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->torchio) (2025.10.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.9->torchio)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.9->torchio)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.9->torchio)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.9->torchio)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.9->torchio)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.9->torchio)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.9->torchio)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.9->torchio)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.9->torchio)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->torchio) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->torchio) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->torchio) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.9->torchio)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->torchio) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->torchio) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.9->torchio) (1.3.0)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.1->torchio) (8.3.0)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.1->torchio) (1.5.4)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10->torchio) (0.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.9->torchio) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20->torchio) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20->torchio) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20->torchio) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.20->torchio) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.20->torchio) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.20->torchio) (2024.2.0)\nDownloading torchio-0.21.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.9/193.9 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchio\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchio-0.21.0\nCollecting focal_loss_torch\n  Downloading focal_loss_torch-0.1.2-py3-none-any.whl.metadata (2.2 kB)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from focal_loss_torch) (2.6.0+cu124)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from focal_loss_torch) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->focal_loss_torch) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->focal_loss_torch) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->focal_loss_torch) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->focal_loss_torch) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->focal_loss_torch) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->focal_loss_torch) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->focal_loss_torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->focal_loss_torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->focal_loss_torch) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->focal_loss_torch) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->focal_loss_torch) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->focal_loss_torch) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->focal_loss_torch) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->focal_loss_torch) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->focal_loss_torch) (2024.2.0)\nDownloading focal_loss_torch-0.1.2-py3-none-any.whl (4.5 kB)\nInstalling collected packages: focal_loss_torch\nSuccessfully installed focal_loss_torch-0.1.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tqdm import tqdm\nfrom PIL import Image\nimport seaborn as sns\nimport math\nimport os\nimport warnings\nimport logging\nimport time\n\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch import nn, einsum\nimport torchio as tio\nimport torch.nn.functional as F\nfrom scipy.ndimage import rotate, zoom\nimport random\nfrom focal_loss.focal_loss import FocalLoss\n\nfrom sklearn.metrics import (\n    confusion_matrix, \n    classification_report, \n    cohen_kappa_score,\n    accuracy_score,\n    precision_recall_fscore_support,\n    roc_auc_score,\n    roc_curve\n)\n\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\n\nwarnings.filterwarnings(\"ignore\", module=\"torchio\")","metadata":{"id":"sFXGokZi-pq9","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T08:48:51.671659Z","iopub.execute_input":"2025-11-20T08:48:51.671923Z","iopub.status.idle":"2025-11-20T08:48:59.551498Z","shell.execute_reply.started":"2025-11-20T08:48:51.671899Z","shell.execute_reply":"2025-11-20T08:48:59.550877Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"id":"LS3RNE2u-prA","outputId":"1a9b9771-9eae-46a8-ea83-bd4d2ae89cb8","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T08:48:59.552184Z","iopub.execute_input":"2025-11-20T08:48:59.552560Z","iopub.status.idle":"2025-11-20T08:48:59.618034Z","shell.execute_reply.started":"2025-11-20T08:48:59.552542Z","shell.execute_reply":"2025-11-20T08:48:59.617141Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    # nếu muốn reproducible tuyệt đối: deterministic=True + benchmark=False\n    # Phát triển kiến trúc: deterministic=False + benchmark=True\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(42)\n\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ng = torch.Generator()\ng.manual_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T08:48:59.619762Z","iopub.execute_input":"2025-11-20T08:48:59.620062Z","iopub.status.idle":"2025-11-20T08:48:59.641138Z","shell.execute_reply.started":"2025-11-20T08:48:59.620040Z","shell.execute_reply":"2025-11-20T08:48:59.640572Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7c80b5415990>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, dataframe, transforms=None):\n        self.df = pd.read_csv(dataframe)\n        self.transforms = transforms\n\n    def __getitem__(self, index):\n        path_object = self.df.loc[index]['mri_path']\n        mri_file = '/workspace/data/SAG_3D_DESS_v2_full/MRI_Numpy/' + path_object\n        mri_dict = np.load(mri_file)\n        mri_object = mri_dict['data']\n\n        mri_object = np.expand_dims(mri_object, 0) # (1 x 120 x 160 x 160)\n        mri_object = self.transforms(mri_object)\n        mri_tensor = torch.tensor(mri_object)\n\n        label = self.df.loc[index]['kl_grade']\n\n        return mri_tensor, label\n\n    def __len__(self):\n        return len(self.df)","metadata":{"id":"jT0Xiitk-prC","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T08:48:59.641800Z","iopub.execute_input":"2025-11-20T08:48:59.642108Z","iopub.status.idle":"2025-11-20T08:48:59.647006Z","shell.execute_reply.started":"2025-11-20T08:48:59.642089Z","shell.execute_reply":"2025-11-20T08:48:59.646218Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"spatial_augment = [\n    tio.RandomAffine(degrees=15, p=0.5),\n    tio.RandomFlip(axes=(0,), flip_probability=0.5),\n]\n\nintensity_augment = {\n    tio.RandomNoise(): 0.25,\n    tio.RandomBiasField(): 0.25,\n    tio.RandomBlur(std=(0,1.5)): 0.25,\n    tio.RandomMotion(): 0.25,\n}\n\n\ntrain_transforms = tio.Compose([\n    tio.Compose(spatial_augment, p=1),\n    tio.OneOf(intensity_augment, p=0.75),\n    tio.RescaleIntensity(out_min_max=(0,1)),\n])\n\nval_transforms = tio.Compose([\n    tio.RescaleIntensity(out_min_max=(0,1)),\n])\n\ntest_transforms = tio.Compose([\n    tio.RescaleIntensity(out_min_max=(0,1)),\n])\n\n\ndf = pd.read_csv('/workspace/data/unified_xray_mri_label.csv')\n\ntrain_df = df[df['subset'] == 'train'].reset_index(drop=True)\nval_df = df[df['subset'] == 'val'].reset_index(drop=True)\ntest_df = df[df['subset'] == 'test'].reset_index(drop=True)\n\ntrain_ds = CustomDataset(train_df, transforms=train_transforms)\nval_ds = CustomDataset(val_df, transforms=val_transforms)\ntest_ds = CustomDataset(test_df, transforms=test_transforms)","metadata":{"id":"sW094pPO-prD","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T08:48:59.647759Z","iopub.execute_input":"2025-11-20T08:48:59.648066Z","iopub.status.idle":"2025-11-20T08:48:59.716025Z","shell.execute_reply.started":"2025-11-20T08:48:59.648049Z","shell.execute_reply":"2025-11-20T08:48:59.715411Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train_loader = DataLoader(\n    train_ds,\n    batch_size=8,\n    shuffle=True,\n    num_workers=8,\n    pin_memory=True,\n    worker_init_fn=seed_worker,\n    generator=g\n)\n\nval_loader = DataLoader(\n    val_ds,\n    batch_size=4,\n    shuffle=False,\n    num_workers=8,\n    pin_memory=True,\n)\n\ntest_loader = DataLoader(\n    test_ds,\n    batch_size=4,\n    shuffle=False,\n    num_workers=8,\n    pin_memory=True,\n)","metadata":{"id":"Xhstvlnz-prE","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T08:48:59.716825Z","iopub.execute_input":"2025-11-20T08:48:59.717074Z","iopub.status.idle":"2025-11-20T08:48:59.722410Z","shell.execute_reply.started":"2025-11-20T08:48:59.717055Z","shell.execute_reply":"2025-11-20T08:48:59.721605Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# for mri, label in train_loader:\n#     print(mri.shape)\n#     print(label.shape)\n#     print(torch.max(mri), torch.min(mri))\n#     break","metadata":{"id":"hhVY4ta--prE","outputId":"e2d7023e-e4fd-4241-cf35-ae71ff0ac39e","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T08:48:59.723368Z","iopub.execute_input":"2025-11-20T08:48:59.723642Z","iopub.status.idle":"2025-11-20T08:48:59.738705Z","shell.execute_reply.started":"2025-11-20T08:48:59.723617Z","shell.execute_reply":"2025-11-20T08:48:59.737964Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Vision Transformer","metadata":{"id":"bHUY4HaM-prF"}},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\n\n# helpers\n\ndef pair(t):\n    return t if isinstance(t, tuple) else (t, t)\n\n# classes\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout = 0.):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass Attention(nn.Module):\n    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n        super().__init__()\n        inner_dim = dim_head *  heads\n        project_out = not (heads == 1 and dim_head == dim)\n\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n\n        self.norm = nn.LayerNorm(dim)\n        self.attend = nn.Softmax(dim = -1)\n        self.dropout = nn.Dropout(dropout)\n\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim),\n            nn.Dropout(dropout)\n        ) if project_out else nn.Identity()\n\n    def forward(self, x):\n        x = self.norm(x)\n        qkv = self.to_qkv(x).chunk(3, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n\n        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n\n        attn = self.attend(dots)\n        attn = self.dropout(attn)\n\n        out = torch.matmul(attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass Transformer(nn.Module):\n    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout,),\n                FeedForward(dim, mlp_dim, dropout = dropout)\n            ]))\n\n    def forward(self, x):\n        for attn, ff in self.layers:\n            x = attn(x) + x\n            x = ff(x) + x\n\n        return self.norm(x)\n\nclass VisionTransformer(nn.Module):\n    def __init__(self,\n                 *,\n                 image_size,\n                 image_patch_size,\n                 frames,\n                 frame_patch_size,\n                 num_classes,\n                 dim, depth,\n                 heads,\n                 mlp_dim,\n                 pool = 'cls',\n                 channels = 3,\n                 dim_head = 64,\n                 dropout = 0.,\n                 emb_dropout = 0.,\n                 pretrain_path=None):\n        super().__init__()\n        image_height, image_width = pair(image_size)\n        patch_height, patch_width = pair(image_patch_size)\n\n        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n        assert frames % frame_patch_size == 0, 'Frames must be divisible by frame patch size'\n\n        num_patches = (image_height // patch_height) * (image_width // patch_width) * (frames // frame_patch_size)\n        patch_dim = channels * patch_height * patch_width * frame_patch_size\n        self.num_patches = num_patches\n        self.image_size = image_size\n        self.image_patch_size = image_patch_size\n        self.frames = frames\n        self.frame_patch_size=frame_patch_size\n\n        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n\n        self.conv_proj = nn.Sequential(\n            nn.Conv3d(channels, dim, kernel_size=(frame_patch_size, image_patch_size, image_patch_size), stride=(frame_patch_size, image_patch_size, image_patch_size),\n        ))\n\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n        self.dropout = nn.Dropout(emb_dropout)\n\n        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n\n        self.pool = pool\n        self.to_latent = nn.Identity()\n\n        self.mlp_head = nn.Linear(dim, num_classes)\n\n        if pretrain_path is not None:\n            self.load_pretrain(pretrain_path)\n            print(f'Load pretrained {pretrain_path} sucessfully!')\n\n    def load_pretrain(self, pretrain_path):\n        jax_dict = torch.load(pretrain_path, map_location='cpu')\n        new_dict = {}\n\n        def interpolate_pos_embedding(pre_pos_embed):\n            cls_token, pretrained_pos_embed = pre_pos_embed[:, :1, :], pre_pos_embed[:, 1:, :]  # [1, 1, 768], [1, 196, 768]\n            new_num_patches = self.num_patches # 1000\n            old_num_patches = int(pretrained_pos_embed.shape[1] ** 0.5) # 14\n            pretrained_pos_embed = pretrained_pos_embed.reshape(1, old_num_patches, old_num_patches, -1).permute(0, 3, 1, 2)  # [1, 768, 14, 14]\n            pretrained_pos_embed = pretrained_pos_embed.unsqueeze(2)  # [1, 768, 1, 14, 14]\n            new_size = round(new_num_patches ** (1/3))\n            pretrained_pos_embed = F.interpolate(pretrained_pos_embed, size=(new_size, new_size, new_size), mode='trilinear', align_corners=False)  # [1, 768, 10, 10, 10]\n            pretrained_pos_embed = pretrained_pos_embed.permute(0, 2, 3, 4, 1).reshape(1, new_size*new_size*new_size, -1) # [1,1000, 768]\n            new_pos_embed = torch.cat([cls_token, pretrained_pos_embed], dim=1)\n            return new_pos_embed\n\n        def mean_kernel(patch_emb_weight):\n            patch_emb_weight = patch_emb_weight.mean(dim=1, keepdim=True)  # Shape: [768, 1, 16, 16]\n            depth = self.conv_proj[0].weight.shape[2]\n            patch_emb_weight = patch_emb_weight.unsqueeze(2).repeat(1, 1, depth, 1, 1)  # Shape: [768, 1, 12, 16, 16]\n            return patch_emb_weight\n\n        def add_item(key, value):\n            key = key.replace('blocks', 'transformer.layers')\n            new_dict[key] = value\n\n        for key, value in jax_dict.items():\n            if key == 'cls_token':\n                new_dict[key] = value\n\n            elif 'norm1' in key:\n                new_key = key.replace('norm1', '0.norm')\n                add_item(new_key, value)\n            elif 'attn.qkv' in key:\n                new_key = key.replace('attn.qkv', '0.to_qkv')\n                add_item(new_key, value)\n            elif 'attn.proj' in key:\n                new_key = key.replace('attn.proj', '0.to_out.0')\n                add_item(new_key, value)\n            elif 'norm2' in key:\n                new_key = key.replace('norm2', '1.net.0')\n                add_item(new_key, value)\n            elif 'mlp.fc1' in key:\n                new_key = key.replace('mlp.fc1', '1.net.1')\n                add_item(new_key, value)\n            elif 'mlp.fc2' in key:\n                new_key = key.replace('mlp.fc2', '1.net.4')\n                add_item(new_key, value)\n            elif 'patch_embed.proj.weight' in key:\n                new_key = key.replace('patch_embed.proj.weight', 'conv_proj.0.weight')\n                value = mean_kernel(value)\n                add_item(new_key, value)\n            elif 'patch_embed.proj.bias' in key:\n                new_key = key.replace('patch_embed.proj.bias', 'conv_proj.0.bias')\n                add_item(new_key, value)\n            elif key == 'pos_embed':\n                value = interpolate_pos_embedding(value)\n                add_item('pos_embedding', value)\n            elif key == 'norm.weight':\n                add_item('transformer.norm.weight', value)\n            elif key == 'norm.bias':\n                add_item('transformer.norm.bias', value)\n\n        self.load_state_dict(new_dict, strict=False)\n\n\n    def forward(self, img):\n        x = self.conv_proj(img)\n        x = x.flatten(2).transpose(1,2) # [B, N, C]\n        b, n, _ = x.shape\n\n        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x += self.pos_embedding[:, :(n + 1)]\n        x = self.dropout(x)\n\n        x = self.transformer(x)\n\n        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n\n        x = self.to_latent(x)\n        return self.mlp_head(x)","metadata":{"id":"slg54Iqg-prH","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T09:21:35.628977Z","iopub.execute_input":"2025-11-20T09:21:35.629617Z","iopub.status.idle":"2025-11-20T09:21:35.664219Z","shell.execute_reply.started":"2025-11-20T09:21:35.629592Z","shell.execute_reply":"2025-11-20T09:21:35.663400Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import timm\nimport torch\n\npretrained = timm.create_model('vit_base_patch16_224.orig_in21k', pretrained=True)\n\ntorch.save(pretrained.state_dict(), \"vit_base_patch16_224_in21k.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T09:21:36.481118Z","iopub.execute_input":"2025-11-20T09:21:36.481683Z","iopub.status.idle":"2025-11-20T09:21:38.749387Z","shell.execute_reply.started":"2025-11-20T09:21:36.481658Z","shell.execute_reply":"2025-11-20T09:21:38.748416Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"model = VisionTransformer(\n    image_size=160,\n    image_patch_size=16,\n    frames = 120,\n    frame_patch_size = 12,\n    depth=12,\n    heads=12,\n    dim=768,\n    mlp_dim=3072,\n    dropout=0.2,\n    emb_dropout=0.1,\n    channels = 1,\n    num_classes = 5,\n    freeze_vit = True,\n    pool = 'cls',\n    pretrain_path = './pretrained/vit_base_patch16_224_in21k.pth',\n)\nmodel.to(device)","metadata":{"id":"puUXqeV7-prL","outputId":"756a0f70-ad23-4ec1-f954-81a546581ced","scrolled":true,"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T09:21:41.474138Z","iopub.execute_input":"2025-11-20T09:21:41.474464Z","iopub.status.idle":"2025-11-20T09:21:43.161447Z","shell.execute_reply.started":"2025-11-20T09:21:41.474441Z","shell.execute_reply":"2025-11-20T09:21:43.160783Z"}},"outputs":[{"name":"stdout","text":"Load pretrained /kaggle/working/vit_base_patch16_224_in21k.pth sucessfully!\nFreezing pretrained ViT components...\nTrainable: transformer.layers.0.1.adapter_layer_norm_before.weight | Shape: [768]               \nTrainable: transformer.layers.0.1.adapter_layer_norm_before.bias | Shape: [768]               \nTrainable: transformer.layers.0.1.expand.weight               | Shape: [3072, 768]         \nTrainable: transformer.layers.0.1.expand.bias                 | Shape: [3072]              \nTrainable: transformer.layers.0.1.dw_conv.weight              | Shape: [3072, 1, 3, 3, 3]  \nTrainable: transformer.layers.0.1.bn.weight                   | Shape: [3072]              \nTrainable: transformer.layers.0.1.bn.bias                     | Shape: [3072]              \nTrainable: transformer.layers.0.1.project.weight              | Shape: [768, 3072]         \nTrainable: transformer.layers.0.1.project.bias                | Shape: [768]               \nTrainable: transformer.layers.1.1.adapter_layer_norm_before.weight | Shape: [768]               \nTrainable: transformer.layers.1.1.adapter_layer_norm_before.bias | Shape: [768]               \nTrainable: transformer.layers.1.1.expand.weight               | Shape: [3072, 768]         \nTrainable: transformer.layers.1.1.expand.bias                 | Shape: [3072]              \nTrainable: transformer.layers.1.1.dw_conv.weight              | Shape: [3072, 1, 3, 3, 3]  \nTrainable: transformer.layers.1.1.bn.weight                   | Shape: [3072]              \nTrainable: transformer.layers.1.1.bn.bias                     | Shape: [3072]              \nTrainable: transformer.layers.1.1.project.weight              | Shape: [768, 3072]         \nTrainable: transformer.layers.1.1.project.bias                | Shape: [768]               \nTrainable: transformer.layers.2.1.adapter_layer_norm_before.weight | Shape: [768]               \nTrainable: transformer.layers.2.1.adapter_layer_norm_before.bias | Shape: [768]               \nTrainable: transformer.layers.2.1.expand.weight               | Shape: [3072, 768]         \nTrainable: transformer.layers.2.1.expand.bias                 | Shape: [3072]              \nTrainable: transformer.layers.2.1.dw_conv.weight              | Shape: [3072, 1, 3, 3, 3]  \nTrainable: transformer.layers.2.1.bn.weight                   | Shape: [3072]              \nTrainable: transformer.layers.2.1.bn.bias                     | Shape: [3072]              \nTrainable: transformer.layers.2.1.project.weight              | Shape: [768, 3072]         \nTrainable: transformer.layers.2.1.project.bias                | Shape: [768]               \nTrainable: transformer.layers.3.1.adapter_layer_norm_before.weight | Shape: [768]               \nTrainable: transformer.layers.3.1.adapter_layer_norm_before.bias | Shape: [768]               \nTrainable: transformer.layers.3.1.expand.weight               | Shape: [3072, 768]         \nTrainable: transformer.layers.3.1.expand.bias                 | Shape: [3072]              \nTrainable: transformer.layers.3.1.dw_conv.weight              | Shape: [3072, 1, 3, 3, 3]  \nTrainable: transformer.layers.3.1.bn.weight                   | Shape: [3072]              \nTrainable: transformer.layers.3.1.bn.bias                     | Shape: [3072]              \nTrainable: transformer.layers.3.1.project.weight              | Shape: [768, 3072]         \nTrainable: transformer.layers.3.1.project.bias                | Shape: [768]               \nTrainable: transformer.layers.4.1.adapter_layer_norm_before.weight | Shape: [768]               \nTrainable: transformer.layers.4.1.adapter_layer_norm_before.bias | Shape: [768]               \nTrainable: transformer.layers.4.1.expand.weight               | Shape: [3072, 768]         \nTrainable: transformer.layers.4.1.expand.bias                 | Shape: [3072]              \nTrainable: transformer.layers.4.1.dw_conv.weight              | Shape: [3072, 1, 3, 3, 3]  \nTrainable: transformer.layers.4.1.bn.weight                   | Shape: [3072]              \nTrainable: transformer.layers.4.1.bn.bias                     | Shape: [3072]              \nTrainable: transformer.layers.4.1.project.weight              | Shape: [768, 3072]         \nTrainable: transformer.layers.4.1.project.bias                | Shape: [768]               \nTrainable: transformer.layers.5.1.adapter_layer_norm_before.weight | Shape: [768]               \nTrainable: transformer.layers.5.1.adapter_layer_norm_before.bias | Shape: [768]               \nTrainable: transformer.layers.5.1.expand.weight               | Shape: [3072, 768]         \nTrainable: transformer.layers.5.1.expand.bias                 | Shape: [3072]              \nTrainable: transformer.layers.5.1.dw_conv.weight              | Shape: [3072, 1, 3, 3, 3]  \nTrainable: transformer.layers.5.1.bn.weight                   | Shape: [3072]              \nTrainable: transformer.layers.5.1.bn.bias                     | Shape: [3072]              \nTrainable: transformer.layers.5.1.project.weight              | Shape: [768, 3072]         \nTrainable: transformer.layers.5.1.project.bias                | Shape: [768]               \nTrainable: transformer.layers.6.1.adapter_layer_norm_before.weight | Shape: [768]               \nTrainable: transformer.layers.6.1.adapter_layer_norm_before.bias | Shape: [768]               \nTrainable: transformer.layers.6.1.expand.weight               | Shape: [3072, 768]         \nTrainable: transformer.layers.6.1.expand.bias                 | Shape: [3072]              \nTrainable: transformer.layers.6.1.dw_conv.weight              | Shape: [3072, 1, 3, 3, 3]  \nTrainable: transformer.layers.6.1.bn.weight                   | Shape: [3072]              \nTrainable: transformer.layers.6.1.bn.bias                     | Shape: [3072]              \nTrainable: transformer.layers.6.1.project.weight              | Shape: [768, 3072]         \nTrainable: transformer.layers.6.1.project.bias                | Shape: [768]               \nTrainable: transformer.layers.7.1.adapter_layer_norm_before.weight | Shape: [768]               \nTrainable: transformer.layers.7.1.adapter_layer_norm_before.bias | Shape: [768]               \nTrainable: transformer.layers.7.1.expand.weight               | Shape: [3072, 768]         \nTrainable: transformer.layers.7.1.expand.bias                 | Shape: [3072]              \nTrainable: transformer.layers.7.1.dw_conv.weight              | Shape: [3072, 1, 3, 3, 3]  \nTrainable: transformer.layers.7.1.bn.weight                   | Shape: [3072]              \nTrainable: transformer.layers.7.1.bn.bias                     | Shape: [3072]              \nTrainable: transformer.layers.7.1.project.weight              | Shape: [768, 3072]         \nTrainable: transformer.layers.7.1.project.bias                | Shape: [768]               \nTrainable: transformer.layers.8.1.adapter_layer_norm_before.weight | Shape: [768]               \nTrainable: transformer.layers.8.1.adapter_layer_norm_before.bias | Shape: [768]               \nTrainable: transformer.layers.8.1.expand.weight               | Shape: [3072, 768]         \nTrainable: transformer.layers.8.1.expand.bias                 | Shape: [3072]              \nTrainable: transformer.layers.8.1.dw_conv.weight              | Shape: [3072, 1, 3, 3, 3]  \nTrainable: transformer.layers.8.1.bn.weight                   | Shape: [3072]              \nTrainable: transformer.layers.8.1.bn.bias                     | Shape: [3072]              \nTrainable: transformer.layers.8.1.project.weight              | Shape: [768, 3072]         \nTrainable: transformer.layers.8.1.project.bias                | Shape: [768]               \nTrainable: transformer.layers.9.1.adapter_layer_norm_before.weight | Shape: [768]               \nTrainable: transformer.layers.9.1.adapter_layer_norm_before.bias | Shape: [768]               \nTrainable: transformer.layers.9.1.expand.weight               | Shape: [3072, 768]         \nTrainable: transformer.layers.9.1.expand.bias                 | Shape: [3072]              \nTrainable: transformer.layers.9.1.dw_conv.weight              | Shape: [3072, 1, 3, 3, 3]  \nTrainable: transformer.layers.9.1.bn.weight                   | Shape: [3072]              \nTrainable: transformer.layers.9.1.bn.bias                     | Shape: [3072]              \nTrainable: transformer.layers.9.1.project.weight              | Shape: [768, 3072]         \nTrainable: transformer.layers.9.1.project.bias                | Shape: [768]               \nTrainable: transformer.layers.10.1.adapter_layer_norm_before.weight | Shape: [768]               \nTrainable: transformer.layers.10.1.adapter_layer_norm_before.bias | Shape: [768]               \nTrainable: transformer.layers.10.1.expand.weight              | Shape: [3072, 768]         \nTrainable: transformer.layers.10.1.expand.bias                | Shape: [3072]              \nTrainable: transformer.layers.10.1.dw_conv.weight             | Shape: [3072, 1, 3, 3, 3]  \nTrainable: transformer.layers.10.1.bn.weight                  | Shape: [3072]              \nTrainable: transformer.layers.10.1.bn.bias                    | Shape: [3072]              \nTrainable: transformer.layers.10.1.project.weight             | Shape: [768, 3072]         \nTrainable: transformer.layers.10.1.project.bias               | Shape: [768]               \nTrainable: transformer.layers.11.1.adapter_layer_norm_before.weight | Shape: [768]               \nTrainable: transformer.layers.11.1.adapter_layer_norm_before.bias | Shape: [768]               \nTrainable: transformer.layers.11.1.expand.weight              | Shape: [3072, 768]         \nTrainable: transformer.layers.11.1.expand.bias                | Shape: [3072]              \nTrainable: transformer.layers.11.1.dw_conv.weight             | Shape: [3072, 1, 3, 3, 3]  \nTrainable: transformer.layers.11.1.bn.weight                  | Shape: [3072]              \nTrainable: transformer.layers.11.1.bn.bias                    | Shape: [3072]              \nTrainable: transformer.layers.11.1.project.weight             | Shape: [768, 3072]         \nTrainable: transformer.layers.11.1.project.bias               | Shape: [768]               \nTrainable: mlp_head.weight                                    | Shape: [5, 768]            \nTrainable: mlp_head.bias                                      | Shape: [5]                 \nTrainable parameters: 57,760,517 (57.76M)\nFrozen parameters: 88,157,952 (88.16M)\nTrainable ratio: 39.58%\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"VisionTransformer(\n  (conv_proj): Sequential(\n    (0): Conv3d(1, 768, kernel_size=(12, 16, 16), stride=(12, 16, 16))\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (transformer): Transformer(\n    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (layers): ModuleList(\n      (0-11): 12 x ModuleList(\n        (0): Attention(\n          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attend): Softmax(dim=-1)\n          (dropout): Dropout(p=0.2, inplace=False)\n          (to_qkv): Linear(in_features=768, out_features=2304, bias=False)\n          (to_out): Sequential(\n            (0): Linear(in_features=768, out_features=768, bias=True)\n            (1): Dropout(p=0.2, inplace=False)\n          )\n        )\n        (1): InvertedResidualAdapter(\n          (adapter_layer_norm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (expand): Linear(in_features=768, out_features=3072, bias=True)\n          (dw_conv): Conv3d(3072, 3072, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=3072, bias=False)\n          (bn): BatchNorm3d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (project): Linear(in_features=3072, out_features=768, bias=True)\n          (activation): ReLU6(inplace=True)\n          (dropout): Dropout(p=0.2, inplace=False)\n        )\n        (2): FeedForward(\n          (net): Sequential(\n            (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (1): Linear(in_features=768, out_features=3072, bias=True)\n            (2): GELU(approximate='none')\n            (3): Dropout(p=0.2, inplace=False)\n            (4): Linear(in_features=3072, out_features=768, bias=True)\n            (5): Dropout(p=0.2, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (to_latent): Identity()\n  (mlp_head): Linear(in_features=768, out_features=5, bias=True)\n)"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"count_freeze = 0\ncount_tuning = 0\n\nfor name, param in model.named_parameters():\n    if param.requires_grad == True:\n        count_tuning += 1\n    else:\n        count_freeze += 1\n\nprint(f'There are {count_tuning} trainable params.')\nprint(f'There are {count_freeze} freeze params')\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total trainable parameters: {total_params}\")","metadata":{"id":"KEKosbZj-prM","outputId":"656521e7-dda5-46ef-fa06-bc4de303f354","scrolled":true,"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T09:22:44.769910Z","iopub.execute_input":"2025-11-20T09:22:44.770202Z","iopub.status.idle":"2025-11-20T09:22:44.777074Z","shell.execute_reply.started":"2025-11-20T09:22:44.770179Z","shell.execute_reply":"2025-11-20T09:22:44.776396Z"}},"outputs":[{"name":"stdout","text":"There are 110 trainable params.\nThere are 138 freeze params\nTotal trainable parameters: 57760517\nGAViKO go go!\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"# Define Loss","metadata":{"id":"ceUKvFD7-prN"}},{"cell_type":"code","source":"from focal_loss.focal_loss import FocalLoss\n\n# alpha = torch.FloatTensor([2.65, 5.39, 3.83, 7.03, 29.67]).to(device)\ncriterion = FocalLoss(gamma=1.2)","metadata":{"id":"HaJuvQ7Q-prN","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T06:59:37.295377Z","iopub.execute_input":"2025-11-20T06:59:37.295717Z","iopub.status.idle":"2025-11-20T06:59:37.323376Z","shell.execute_reply.started":"2025-11-20T06:59:37.295686Z","shell.execute_reply":"2025-11-20T06:59:37.322118Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainable_params = [p for p in model.parameters() if p.requires_grad]\n\noptimizer = torch.optim.Adam(trainable_params, lr=1e-4)\n\nfrom torch.optim.lr_scheduler import OneCycleLR\nsteps_per_epoch = len(train_loader)\nnum_epochs = 30\ntotal_steps = steps_per_epoch * num_epochs\nscheduler = OneCycleLR(\n    optimizer,\n    max_lr=3e-4,  # learning rate cao nhất\n    total_steps=total_steps,\n    pct_start=0.3,  # % số bước dành cho giai đoạn tăng lr (warmup)\n    div_factor=10.0,  # lr_start = max_lr / div_factor\n    final_div_factor=1000.0,  # lr_final = lr_start / final_div_factor\n    anneal_strategy='cos',  # sử dụng cosine annealing\n    three_phase=False  # không dùng 3 giai đoạn (chỉ dùng 2: lên-xuống)\n)","metadata":{"id":"qED4EJn_-prO","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T06:59:37.324551Z","iopub.execute_input":"2025-11-20T06:59:37.325053Z","iopub.status.idle":"2025-11-20T06:59:37.348111Z","shell.execute_reply.started":"2025-11-20T06:59:37.325018Z","shell.execute_reply":"2025-11-20T06:59:37.346798Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training Setup","metadata":{"id":"lt2qc_Q9-prO"}},{"cell_type":"code","source":"# =============================================================================\n# SETUP\n# =============================================================================\nsave_dir = \"./output\"\nos.makedirs(save_dir, exist_ok=True)\n\n# Logging setup\nlog_file = os.path.join(save_dir, 'training_log.txt')\nlogging.basicConfig(\n    filename=log_file,\n    level=logging.INFO,\n    format='%(asctime)s - %(message)s'\n)\n\n# Also log to console\nconsole = logging.StreamHandler()\nconsole.setLevel(logging.INFO)\nlogging.getLogger('').addHandler(console)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T06:59:37.34922Z","iopub.execute_input":"2025-11-20T06:59:37.349518Z","iopub.status.idle":"2025-11-20T06:59:37.371481Z","shell.execute_reply.started":"2025-11-20T06:59:37.349494Z","shell.execute_reply":"2025-11-20T06:59:37.370327Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TrainingState:\n    \"\"\"Track training state\"\"\"\n    def __init__(self):\n        self.val_acc_max = 0.0\n        self.val_kappa_max = 0.0\n        self.current_epoch = 0\n        self.epoch_since_improvement = 0\n        self.val_loss = 0\n        \n        # Metrics history\n        self.train_loss_history = []\n        self.train_acc_history = []\n        self.val_loss_history = []\n        self.val_acc_history = []\n        self.val_kappa_history = []\n        self.lr_history = []\n        \n        # Best metrics\n        self.best_epoch = 0\n        self.best_model_path = None\n\nstate = TrainingState()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T07:00:52.369552Z","iopub.execute_input":"2025-11-20T07:00:52.37028Z","iopub.status.idle":"2025-11-20T07:00:52.377557Z","shell.execute_reply.started":"2025-11-20T07:00:52.370223Z","shell.execute_reply":"2025-11-20T07:00:52.376491Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training Utils","metadata":{}},{"cell_type":"code","source":"def atomic_save(obj, path):\n    \"\"\"Atomic save to prevent corruption\"\"\"\n    tmp = path + \".tmp\"\n    torch.save(obj, tmp)\n    os.replace(tmp, path)  # Atomic on Unix\n    logging.info(f\"Model saved to {path}\")\n\ndef compute_metrics(predictions, labels, num_classes=5):\n    \"\"\"\n    Compute comprehensive metrics\n    \n    Returns:\n        dict with accuracy, kappa, per-class accuracy, confusion matrix\n    \"\"\"\n    # Accuracy\n    accuracy = (predictions == labels).sum().item() / len(labels)\n    \n    # Cohen's Kappa \n    kappa = cohen_kappa_score(labels, predictions, weights='quadratic')\n    \n    # Confusion matrix\n    cm = confusion_matrix(labels, predictions, labels=range(num_classes))\n    \n    # Per-class accuracy\n    per_class_acc = cm.diagonal() / cm.sum(axis=1)\n    \n    return {\n        'accuracy': accuracy,\n        'kappa': kappa,\n        'confusion_matrix': cm,\n        'per_class_accuracy': per_class_acc\n    }\n\ndef log_metrics(phase, epoch, loss, metrics, lr=None):\n    \"\"\"Log metrics to console and file\"\"\"\n    msg = f\"\\n{'='*80}\\n\"\n    msg += f\"Epoch {epoch + 1} - {phase.upper()}\\n\"\n    msg += f\"{'='*80}\\n\"\n    msg += f\"Loss: {loss:.4f}\\n\"\n    msg += f\"Accuracy: {metrics['accuracy']*100:.2f}%\\n\"\n    msg += f\"Kappa Score: {metrics['kappa']:.4f}\\n\"\n    \n    if lr is not None:\n        msg += f\"Learning Rate: {lr:.6f}\\n\"\n    \n    msg += \"\\nPer-Class Accuracy:\\n\"\n    for i, acc in enumerate(metrics['per_class_accuracy']):\n        msg += f\"  KL Grade {i}: {acc*100:.2f}%\\n\"\n    \n    msg += f\"{'='*80}\\n\"\n    \n    print(msg)\n    logging.info(msg)\n\ndef plot_training_curves(state, save_path):\n    \"\"\"Plot training curves\"\"\"\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    \n    epochs = range(1, len(state.train_loss_history) + 1)\n    \n    # Loss\n    axes[0, 0].plot(epochs, state.train_loss_history, 'b-', label='Train Loss')\n    axes[0, 0].plot(epochs, state.val_loss_history, 'r-', label='Val Loss')\n    axes[0, 0].set_xlabel('Epoch')\n    axes[0, 0].set_ylabel('Loss')\n    axes[0, 0].set_title('Loss Curves')\n    axes[0, 0].legend()\n    axes[0, 0].grid(True)\n    \n    # Accuracy\n    axes[0, 1].plot(epochs, state.train_acc_history, 'b-', label='Train Acc')\n    axes[0, 1].plot(epochs, state.val_acc_history, 'r-', label='Val Acc')\n    axes[0, 1].set_xlabel('Epoch')\n    axes[0, 1].set_ylabel('Accuracy')\n    axes[0, 1].set_title('Accuracy Curves')\n    axes[0, 1].legend()\n    axes[0, 1].grid(True)\n    \n    # Kappa\n    axes[1, 0].plot(epochs, state.val_kappa_history, 'g-', label='Val Kappa')\n    axes[1, 0].set_xlabel('Epoch')\n    axes[1, 0].set_ylabel('Kappa Score')\n    axes[1, 0].set_title('Kappa Score (Validation)')\n    axes[1, 0].legend()\n    axes[1, 0].grid(True)\n    \n    # Learning Rate\n    axes[1, 1].plot(epochs, state.lr_history, 'purple', label='Learning Rate')\n    axes[1, 1].set_xlabel('Epoch')\n    axes[1, 1].set_ylabel('Learning Rate')\n    axes[1, 1].set_title('Learning Rate Schedule')\n    axes[1, 1].set_yscale('log')\n    axes[1, 1].legend()\n    axes[1, 1].grid(True)\n    \n    plt.tight_layout()\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    plt.close()\n    logging.info(f\"Training curves saved to {save_path}\")\n\ndef plot_confusion_matrix(cm, save_path, normalize=False):\n    \"\"\"Plot confusion matrix\"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    \n    plt.figure(figsize=(10, 8))\n    sns.heatmap(\n        cm,\n        annot=True,\n        fmt='.2f' if normalize else 'd',\n        cmap='Blues',\n        xticklabels=[f'KL {i}' for i in range(5)],\n        yticklabels=[f'KL {i}' for i in range(5)],\n        cbar_kws={'label': 'Normalized Count' if normalize else 'Count'}\n    )\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix' + (' (Normalized)' if normalize else ''))\n    plt.tight_layout()\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    plt.close()\n    logging.info(f\"Confusion matrix saved to {save_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T07:00:56.084312Z","iopub.execute_input":"2025-11-20T07:00:56.084638Z","iopub.status.idle":"2025-11-20T07:00:56.115191Z","shell.execute_reply.started":"2025-11-20T07:00:56.084617Z","shell.execute_reply":"2025-11-20T07:00:56.113577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_one_epoch(model, train_loader, criterion, optimizer, scheduler, device, epoch):\n    \"\"\"Train for one epoch\"\"\"\n    model.train()\n    \n    running_loss = 0.0\n    all_predictions = []\n    all_labels = []\n    \n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1} [TRAIN]\")\n    \n    for batch_idx, (batch, labels) in enumerate(pbar):\n        # Get data\n        batch = batch.to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        optimizer.zero_grad()\n        logits = model(batch)\n        \n        m = torch.nn.Softmax(dim=-1)\n        # Compute loss\n        loss = criterion(m(logits), labels)\n\n        # Backward pass\n        loss.backward()\n\n        # Gradient clipping (optional but recommended)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        optimizer.step()\n\n        # Update learning rate (if using OneCycleLR or similar)\n        if scheduler is not None and hasattr(scheduler, 'step') and not isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n            scheduler.step()\n        \n        # Track metrics\n        running_loss += loss.item() * batch.size(0)\n        predictions = torch.argmax(logits, dim=1).cpu().numpy()\n        all_predictions.extend(predictions)\n        all_labels.extend(labels.cpu().numpy())\n        \n        # Update progress bar\n        current_loss = running_loss / ((batch_idx + 1) * train_loader.batch_size)\n        pbar.set_postfix({'loss': f'{current_loss:.4f}'})\n    \n    # Compute epoch metrics\n    epoch_loss = running_loss / len(train_loader.dataset)\n    metrics = compute_metrics(\n        np.array(all_predictions),\n        np.array(all_labels)\n    )\n    \n    return epoch_loss, metrics\n\ndef validate(model, val_loader, criterion, device, epoch):\n    \"\"\"Validate model\"\"\"\n    model.eval()\n    \n    running_loss = 0.0\n    all_predictions = []\n    all_labels = []\n    \n    pbar = tqdm(val_loader, desc=f\"Epoch {epoch + 1} [VAL]\")\n    \n    with torch.no_grad():\n        for batch, labels in pbar:\n            # Get data\n            batch = batch.to(device)\n            labels = labels.to(device)\n            \n            # Forward pass\n            logits = model(batch)\n            m = torch.nn.Softmax(dim=-1)\n            # Compute loss\n            loss = criterion(m(logits), labels)\n            \n            # Track metrics\n            running_loss += loss.item() * batch.size(0)\n            predictions = torch.argmax(logits, dim=1).cpu().numpy()\n            all_predictions.extend(predictions)\n            all_labels.extend(labels.cpu().numpy())\n            \n            # Update progress bar\n            current_loss = running_loss / len(all_labels)\n            pbar.set_postfix({'loss': f'{current_loss:.4f}'})\n    \n    # Compute epoch metrics\n    epoch_loss = running_loss / len(val_loader.dataset)\n    metrics = compute_metrics(\n        np.array(all_predictions),\n        np.array(all_labels)\n    )\n    \n    return epoch_loss, metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T07:01:05.88995Z","iopub.execute_input":"2025-11-20T07:01:05.890445Z","iopub.status.idle":"2025-11-20T07:01:05.907338Z","shell.execute_reply.started":"2025-11-20T07:01:05.890417Z","shell.execute_reply":"2025-11-20T07:01:05.905925Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(\n    model,\n    train_loader,\n    val_loader,\n    criterion,\n    optimizer,\n    scheduler,\n    device,\n    num_epochs=100,\n    patience=10,\n    save_dir='./checkpoints',\n    early_stop_metric='accuracy'  # 'accuracy' or 'kappa'\n):\n    \"\"\"\n    Main training loop\n    \n    Args:\n        model: CVPT model\n        train_loader: Training dataloader\n        val_loader: Validation dataloader\n        criterion: Loss function\n        optimizer: Optimizer\n        scheduler: Learning rate scheduler\n        device: Device to train on\n        num_epochs: Maximum number of epochs\n        patience: Early stopping patience\n        save_dir: Directory to save checkpoints\n        early_stop_metric: Metric to use for early stopping ('accuracy' or 'kappa')\n    \"\"\"\n    \n    os.makedirs(save_dir, exist_ok=True)\n    \n    logging.info(f\"\\n{'='*80}\")\n    logging.info(\"STARTING TRAINING\")\n    logging.info(f\"{'='*80}\")\n    logging.info(f\"Device: {device}\")\n    logging.info(f\"Number of epochs: {num_epochs}\")\n    logging.info(f\"Patience: {patience}\")\n    logging.info(f\"Early stop metric: {early_stop_metric}\")\n    logging.info(f\"Save directory: {save_dir}\")\n    logging.info(f\"{'='*80}\\n\")\n    \n    global state\n    \n    for epoch in range(num_epochs):\n        epoch_start_time = time.time()\n        \n        # ========== TRAINING ==========\n        train_loss, train_metrics = train_one_epoch(\n            model, train_loader, criterion, optimizer, scheduler, device, epoch\n        )\n        \n        # ========== VALIDATION ==========\n        val_loss, val_metrics = validate(\n            model, val_loader, criterion, device, epoch\n        )\n        \n        # ========== UPDATE SCHEDULER (if ReduceLROnPlateau) ==========\n        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n            scheduler.step(val_loss)\n        \n        # ========== RECORD METRICS ==========\n        current_lr = optimizer.param_groups[0]['lr']\n        \n        state.train_loss_history.append(train_loss)\n        state.train_acc_history.append(train_metrics['accuracy'])\n        state.val_loss_history.append(val_loss)\n        state.val_acc_history.append(val_metrics['accuracy'])\n        state.val_kappa_history.append(val_metrics['kappa'])\n        state.lr_history.append(current_lr)\n        state.current_epoch = epoch\n        \n        # ========== LOG METRICS ==========\n        log_metrics('TRAIN', epoch, train_loss, train_metrics, lr=current_lr)\n        log_metrics('VAL', epoch, val_loss, val_metrics)\n        \n        # ========== SAVE BEST MODEL ==========\n        if early_stop_metric == 'accuracy':\n            current_metric = val_metrics['accuracy']\n            best_metric = state.val_acc_max\n\n        elif early_stop_metric == 'val_loss': \n            current_val_loss = val_loss\n            best_val_loss = state.val_loss\n            \n        else:  # kappa\n            current_metric = val_metrics['kappa']\n            best_metric = state.val_kappa_max\n        \n        if current_metric > best_metric:\n            improvement = current_metric - best_metric\n            logging.info(f\"\\n🎉 NEW BEST MODEL! {early_stop_metric.upper()} improved by {improvement:.4f}\")\n            logging.info(f\"   Previous best: {best_metric:.4f}\")\n            logging.info(f\"   New best: {current_metric:.4f}\\n\")\n            \n            # Update best metrics\n            if early_stop_metric == 'accuracy':\n                state.val_acc_max = current_metric\n            elif early_stop_metric == 'val_loss':\n                state.val_loss = val_loss \n            else:\n                state.val_kappa_max = current_metric\n            \n            state.best_epoch = epoch\n            state.epoch_since_improvement = 0\n\n            checkpoint_state_dict = OrderedDict()\n            for name, param in model.named_parameters():\n                if param.requires_grad: \n                    checkpoint_state_dict[name] = param.data.clone()\n            \n            # Save model checkpoint\n            checkpoint = {\n                'epoch': epoch,\n                'model_state_dict': checkpoint_state_dict,\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n                'train_loss': train_loss,\n                'val_loss': val_loss,\n                'train_metrics': train_metrics,\n                'val_metrics': val_metrics,\n                'state': state.__dict__,\n            }\n            \n            # Save with epoch info\n            model_path = os.path.join(\n                save_dir,\n                f'model_epoch{epoch+1}_{early_stop_metric}{current_metric:.4f}.pt'\n            )\n            atomic_save(checkpoint, model_path)\n            \n            # Save as best.pt\n            best_path = os.path.join(save_dir, \"best.pt\")\n            atomic_save(checkpoint, best_path)\n            state.best_model_path = best_path\n            \n            # Save confusion matrix\n            cm_path = os.path.join(save_dir, f'confusion_matrix_epoch{epoch+1}.png')\n            plot_confusion_matrix(val_metrics['confusion_matrix'], cm_path, normalize=True)\n            \n        else:\n            state.epoch_since_improvement += 1\n            logging.info(f\"\\n⚠️  No improvement for {state.epoch_since_improvement} epoch(s)\")\n            \n            # Early stopping check\n            if state.epoch_since_improvement >= patience:\n                logging.info(f\"\\n🛑 EARLY STOPPING at epoch {epoch + 1}\")\n                logging.info(f\"   Best epoch: {state.best_epoch + 1}\")\n                logging.info(f\"   Best {early_stop_metric}: {best_metric:.4f}\\n\")\n                break\n        \n        # ========== SAVE TRAINING CURVES ==========\n        if (epoch + 1) % 5 == 0 or epoch == 0:\n            curves_path = os.path.join(save_dir, 'training_curves.png')\n            plot_training_curves(state, curves_path)\n        \n        # ========== LOG EPOCH TIME ==========\n        epoch_time = time.time() - epoch_start_time\n        logging.info(f\"⏱️  Epoch time: {epoch_time:.2f}s\\n\")\n        \n        # ========== SAVE PERIODIC CHECKPOINT ==========\n        if (epoch + 1) % 10 == 0:\n            checkpoint_state_dict = OrderedDict()\n            for name, param in model.named_parameters():\n                if param.requires_grad: \n                    checkpoint_state_dict[name] = param.data.clone()\n                    \n            periodic_path = os.path.join(save_dir, f'checkpoint_epoch{epoch+1}.pt')\n            checkpoint = {\n                'epoch': epoch,\n                'model_state_dict': checkpoint_state_dict,\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n                'state': state.__dict__,\n            }\n            atomic_save(checkpoint, periodic_path)\n    \n    # ========== TRAINING COMPLETE ==========\n    logging.info(f\"\\n{'='*80}\")\n    logging.info(\"TRAINING COMPLETE\")\n    logging.info(f\"{'='*80}\")\n    logging.info(f\"Total epochs: {state.current_epoch + 1}\")\n    logging.info(f\"Best epoch: {state.best_epoch + 1}\")\n    logging.info(f\"Best validation accuracy: {state.val_acc_max:.4f}\")\n    logging.info(f\"Best validation kappa: {state.val_kappa_max:.4f}\")\n    logging.info(f\"Best model saved to: {state.best_model_path}\")\n    logging.info(f\"{'='*80}\\n\")\n    \n    # Final plots\n    curves_path = os.path.join(save_dir, 'final_training_curves.png')\n    plot_training_curves(state, curves_path)\n    \n    return state","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T02:06:13.873349Z","iopub.execute_input":"2025-11-20T02:06:13.873714Z","iopub.status.idle":"2025-11-20T02:06:13.893565Z","shell.execute_reply.started":"2025-11-20T02:06:13.873697Z","shell.execute_reply":"2025-11-20T02:06:13.892754Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Trainer","metadata":{}},{"cell_type":"code","source":"final_state = train(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        criterion=criterion,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        device=device,\n        num_epochs=1000,\n        patience=1000,\n        save_dir=save_dir,\n        early_stop_metric='accuracy'  # Use kappa for early stopping\n    )\n    \nprint(\"\\nTraining completed successfully!\")\nprint(f\"Best validation accuracy: {final_state.val_acc_max:.4f}\")\nprint(f\"Best validation kappa: {final_state.val_kappa_max:.4f}\")\nprint(f\"Best model: {final_state.best_model_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T09:42:00.368671Z","iopub.execute_input":"2025-11-19T09:42:00.368866Z","execution_failed":"2025-11-19T09:43:51.18Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation Utils","metadata":{}},{"cell_type":"code","source":"# =============================================================================\n# EVALUATION FUNCTIONS\n# =============================================================================\n\ndef evaluate_model(model, test_loader, criterion, device, return_predictions=False):\n    \"\"\"\n    Comprehensive evaluation on test set\n    \n    Args:\n        model: Trained CVPT model\n        test_loader: Test dataloader\n        criterion: Loss function\n        device: Device to evaluate on\n        return_predictions: Whether to return all predictions\n    \n    Returns:\n        results: Dictionary containing all evaluation metrics\n        predictions_dict: (optional) Dict with predictions, labels, etc.\n    \"\"\"\n    model.eval()\n    \n    running_loss = 0.0\n    all_predictions = []\n    all_labels = []\n    all_logits = []\n    all_probs = []\n    \n    print(f\"\\n{'='*80}\")\n    print(\"EVALUATING ON TEST SET\")\n    print(f\"{'='*80}\\n\")\n    \n    with torch.no_grad():\n        for batch, labels in tqdm(test_loader, desc=\"Evaluating\"):\n            # Get data\n            batch = batch.to(device)\n            labels = labels.to(device)\n            \n            # Forward pass\n            logits = model(batch)\n            m = torch.nn.Softmax(dim=-1)\n            # Compute loss\n            loss = criterion(m(logits), labels)\n            \n            # Get predictions and probabilities\n            probs = torch.softmax(logits, dim=1)\n            predictions = torch.argmax(logits, dim=1)\n            \n            # Store results\n            running_loss += loss.item() * batch.size(0)\n            all_predictions.extend(predictions.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            all_logits.append(logits.cpu().numpy())\n            all_probs.append(probs.cpu().numpy())\n    \n    # Convert to numpy arrays\n    all_predictions = np.array(all_predictions)\n    all_labels = np.array(all_labels)\n    all_logits = np.vstack(all_logits)\n    all_probs = np.vstack(all_probs)\n    \n    # Compute loss\n    test_loss = running_loss / len(test_loader.dataset)\n    \n    # Compute comprehensive metrics\n    results = compute_comprehensive_metrics(\n        predictions=all_predictions,\n        labels=all_labels,\n        probs=all_probs,\n        test_loss=test_loss\n    )\n    \n    if return_predictions:\n        predictions_dict = {\n            'predictions': all_predictions,\n            'labels': all_labels,\n            'logits': all_logits,\n            'probs': all_probs,\n        }\n        return results, predictions_dict\n    \n    return results\n\n\ndef compute_comprehensive_metrics(predictions, labels, probs, test_loss):\n    \"\"\"\n    Compute all evaluation metrics\n    \n    Args:\n        predictions: Predicted classes [N]\n        labels: True labels [N]\n        probs: Predicted probabilities [N, num_classes]\n        test_loss: Test loss value\n    \n    Returns:\n        results: Dictionary with all metrics\n    \"\"\"\n    num_classes = probs.shape[1]\n    \n    results = {\n        'test_loss': test_loss,\n    }\n    \n    # ========== Classification Metrics ==========\n    \n    # Overall accuracy\n    results['accuracy'] = accuracy_score(labels, predictions)\n    \n    # Cohen's Kappa (weighted for ordinal data)\n    results['kappa'] = cohen_kappa_score(labels, predictions, weights='quadratic')\n    results['kappa_linear'] = cohen_kappa_score(labels, predictions, weights='linear')\n    results['kappa_unweighted'] = cohen_kappa_score(labels, predictions)\n    \n    # Precision, Recall, F1-Score (macro and weighted)\n    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n        labels, predictions, average='macro', zero_division=0\n    )\n    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n        labels, predictions, average='weighted', zero_division=0\n    )\n    \n    results['precision_macro'] = precision_macro\n    results['recall_macro'] = recall_macro\n    results['f1_macro'] = f1_macro\n    results['precision_weighted'] = precision_weighted\n    results['recall_weighted'] = recall_weighted\n    results['f1_weighted'] = f1_weighted\n    \n    # Per-class metrics\n    precision_per_class, recall_per_class, f1_per_class, support_per_class = \\\n        precision_recall_fscore_support(labels, predictions, average=None, zero_division=0)\n    \n    results['per_class_metrics'] = {\n        f'KL_{i}': {\n            'precision': precision_per_class[i],\n            'recall': recall_per_class[i],\n            'f1': f1_per_class[i],\n            'support': int(support_per_class[i])\n        }\n        for i in range(num_classes)\n    }\n    \n    # Confusion matrix\n    cm = confusion_matrix(labels, predictions, labels=range(num_classes))\n    results['confusion_matrix'] = cm\n    \n    # Per-class accuracy\n    per_class_acc = cm.diagonal() / cm.sum(axis=1)\n    results['per_class_accuracy'] = {\n        f'KL_{i}': per_class_acc[i] for i in range(num_classes)\n    }\n    \n    # ========== Ordinal Metrics ==========\n    \n    # Mean Absolute Error (MAE) - important for ordinal classification\n    results['mae'] = np.mean(np.abs(predictions - labels))\n    \n    # Mean Squared Error (MSE)\n    results['mse'] = np.mean((predictions - labels) ** 2)\n    \n    # Root Mean Squared Error (RMSE)\n    results['rmse'] = np.sqrt(results['mse'])\n    \n    # Off-by-one accuracy (correct or within 1 grade)\n    off_by_one = np.abs(predictions - labels) <= 1\n    results['off_by_one_accuracy'] = np.mean(off_by_one)\n    \n    # ========== Confidence Metrics ==========\n    \n    # Average confidence (max probability)\n    confidences = np.max(probs, axis=1)\n    results['avg_confidence'] = np.mean(confidences)\n    results['std_confidence'] = np.std(confidences)\n    \n    # Confidence for correct/incorrect predictions\n    correct_mask = predictions == labels\n    if correct_mask.sum() > 0:\n        results['avg_confidence_correct'] = np.mean(confidences[correct_mask])\n    if (~correct_mask).sum() > 0:\n        results['avg_confidence_incorrect'] = np.mean(confidences[~correct_mask])\n    \n    # ========== Calibration Metrics ==========\n    \n    # Expected Calibration Error (ECE)\n    results['ece'] = compute_ece(probs, labels, n_bins=10)\n    \n    # ========== AUC Metrics (One-vs-Rest) ==========\n    \n    try:\n        # Binarize labels for AUC computation\n        from sklearn.preprocessing import label_binarize\n        labels_binarized = label_binarize(labels, classes=range(num_classes))\n        \n        # Compute AUC for each class (One-vs-Rest)\n        auc_per_class = {}\n        for i in range(num_classes):\n            if labels_binarized[:, i].sum() > 0:  # Check if class exists\n                auc = roc_auc_score(labels_binarized[:, i], probs[:, i])\n                auc_per_class[f'KL_{i}'] = auc\n        \n        results['auc_per_class'] = auc_per_class\n        \n        # Macro AUC\n        if len(auc_per_class) > 0:\n            results['auc_macro'] = np.mean(list(auc_per_class.values()))\n    except:\n        pass\n    \n    return results\n\n\ndef compute_ece(probs, labels, n_bins=10):\n    \"\"\"\n    Compute Expected Calibration Error\n    \n    Args:\n        probs: Predicted probabilities [N, C]\n        labels: True labels [N]\n        n_bins: Number of bins\n    \n    Returns:\n        ece: Expected Calibration Error\n    \"\"\"\n    confidences = np.max(probs, axis=1)\n    predictions = np.argmax(probs, axis=1)\n    accuracies = (predictions == labels).astype(float)\n    \n    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n    ece = 0.0\n    \n    for i in range(n_bins):\n        bin_lower = bin_boundaries[i]\n        bin_upper = bin_boundaries[i + 1]\n        \n        # Find samples in this bin\n        in_bin = (confidences > bin_lower) & (confidences <= bin_upper)\n        prop_in_bin = np.mean(in_bin)\n        \n        if prop_in_bin > 0:\n            accuracy_in_bin = np.mean(accuracies[in_bin])\n            avg_confidence_in_bin = np.mean(confidences[in_bin])\n            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n    \n    return ece\n\n\n# =============================================================================\n# VISUALIZATION FUNCTIONS\n# =============================================================================\n\ndef plot_confusion_matrix_detailed(cm, save_path, class_names=None):\n    \"\"\"Plot detailed confusion matrix with annotations\"\"\"\n    if class_names is None:\n        class_names = [f'KL {i}' for i in range(len(cm))]\n    \n    # Normalize\n    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    \n    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n    \n    # Raw counts\n    sns.heatmap(\n        cm,\n        annot=True,\n        fmt='d',\n        cmap='Blues',\n        xticklabels=class_names,\n        yticklabels=class_names,\n        ax=axes[0],\n        cbar_kws={'label': 'Count'}\n    )\n    axes[0].set_xlabel('Predicted Label', fontsize=12)\n    axes[0].set_ylabel('True Label', fontsize=12)\n    axes[0].set_title('Confusion Matrix (Raw Counts)', fontsize=14)\n    \n    # Normalized\n    sns.heatmap(\n        cm_normalized,\n        annot=True,\n        fmt='.2%',\n        cmap='Blues',\n        xticklabels=class_names,\n        yticklabels=class_names,\n        ax=axes[1],\n        cbar_kws={'label': 'Percentage'}\n    )\n    axes[1].set_xlabel('Predicted Label', fontsize=12)\n    axes[1].set_ylabel('True Label', fontsize=12)\n    axes[1].set_title('Confusion Matrix (Normalized)', fontsize=14)\n    \n    plt.tight_layout()\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    plt.close()\n    print(f\"✓ Confusion matrix saved to {save_path}\")\n\n\ndef plot_per_class_metrics(results, save_path):\n    \"\"\"Plot per-class performance metrics\"\"\"\n    per_class = results['per_class_metrics']\n    classes = sorted(per_class.keys())\n    \n    metrics = ['precision', 'recall', 'f1']\n    data = {metric: [per_class[cls][metric] for cls in classes] for metric in metrics}\n    \n    x = np.arange(len(classes))\n    width = 0.25\n    \n    fig, ax = plt.subplots(figsize=(12, 6))\n    \n    for i, metric in enumerate(metrics):\n        offset = (i - 1) * width\n        ax.bar(x + offset, data[metric], width, label=metric.capitalize())\n    \n    ax.set_xlabel('KL Grade', fontsize=12)\n    ax.set_ylabel('Score', fontsize=12)\n    ax.set_title('Per-Class Performance Metrics', fontsize=14)\n    ax.set_xticks(x)\n    ax.set_xticklabels(classes)\n    ax.legend()\n    ax.grid(axis='y', alpha=0.3)\n    ax.set_ylim([0, 1.05])\n    \n    plt.tight_layout()\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    plt.close()\n    print(f\"✓ Per-class metrics plot saved to {save_path}\")\n\n\ndef plot_error_distribution(predictions, labels, save_path):\n    \"\"\"Plot distribution of prediction errors\"\"\"\n    errors = predictions - labels\n    \n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Error histogram\n    axes[0].hist(errors, bins=np.arange(-4.5, 5.5, 1), edgecolor='black', alpha=0.7)\n    axes[0].set_xlabel('Prediction Error (Predicted - True)', fontsize=12)\n    axes[0].set_ylabel('Frequency', fontsize=12)\n    axes[0].set_title('Distribution of Prediction Errors', fontsize=14)\n    axes[0].grid(axis='y', alpha=0.3)\n    axes[0].axvline(x=0, color='red', linestyle='--', linewidth=2, label='Perfect Prediction')\n    axes[0].legend()\n    \n    # Error by true class\n    unique_labels = sorted(np.unique(labels))\n    error_by_class = [errors[labels == label] for label in unique_labels]\n    \n    axes[1].boxplot(error_by_class, labels=[f'KL {i}' for i in unique_labels])\n    axes[1].set_xlabel('True KL Grade', fontsize=12)\n    axes[1].set_ylabel('Prediction Error', fontsize=12)\n    axes[1].set_title('Prediction Error by True Class', fontsize=14)\n    axes[1].grid(axis='y', alpha=0.3)\n    axes[1].axhline(y=0, color='red', linestyle='--', linewidth=2)\n    \n    plt.tight_layout()\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    plt.close()\n    print(f\"✓ Error distribution plot saved to {save_path}\")\n\n\ndef plot_confidence_analysis(probs, predictions, labels, save_path):\n    \"\"\"Analyze prediction confidence\"\"\"\n    confidences = np.max(probs, axis=1)\n    correct = (predictions == labels)\n    \n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n    \n    # 1. Confidence distribution\n    axes[0, 0].hist(confidences, bins=50, edgecolor='black', alpha=0.7)\n    axes[0, 0].set_xlabel('Confidence (Max Probability)', fontsize=12)\n    axes[0, 0].set_ylabel('Frequency', fontsize=12)\n    axes[0, 0].set_title('Distribution of Prediction Confidence', fontsize=14)\n    axes[0, 0].grid(axis='y', alpha=0.3)\n    \n    # 2. Confidence by correctness\n    correct_conf = confidences[correct]\n    incorrect_conf = confidences[~correct]\n    \n    axes[0, 1].hist([correct_conf, incorrect_conf], bins=30, label=['Correct', 'Incorrect'], \n                    edgecolor='black', alpha=0.7)\n    axes[0, 1].set_xlabel('Confidence', fontsize=12)\n    axes[0, 1].set_ylabel('Frequency', fontsize=12)\n    axes[0, 1].set_title('Confidence: Correct vs Incorrect Predictions', fontsize=14)\n    axes[0, 1].legend()\n    axes[0, 1].grid(axis='y', alpha=0.3)\n    \n    # 3. Accuracy vs confidence (reliability diagram)\n    n_bins = 10\n    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n    bin_centers = (bin_boundaries[:-1] + bin_boundaries[1:]) / 2\n    bin_accuracies = []\n    bin_confidences = []\n    bin_counts = []\n    \n    for i in range(n_bins):\n        in_bin = (confidences > bin_boundaries[i]) & (confidences <= bin_boundaries[i + 1])\n        if in_bin.sum() > 0:\n            bin_accuracies.append(correct[in_bin].mean())\n            bin_confidences.append(confidences[in_bin].mean())\n            bin_counts.append(in_bin.sum())\n        else:\n            bin_accuracies.append(0)\n            bin_confidences.append(bin_centers[i])\n            bin_counts.append(0)\n    \n    axes[1, 0].plot([0, 1], [0, 1], 'r--', label='Perfect Calibration')\n    axes[1, 0].plot(bin_confidences, bin_accuracies, 'bo-', label='Model')\n    axes[1, 0].set_xlabel('Confidence', fontsize=12)\n    axes[1, 0].set_ylabel('Accuracy', fontsize=12)\n    axes[1, 0].set_title('Reliability Diagram', fontsize=14)\n    axes[1, 0].legend()\n    axes[1, 0].grid(alpha=0.3)\n    \n    # 4. Confidence by class\n    unique_labels = sorted(np.unique(labels))\n    conf_by_class = [confidences[labels == label] for label in unique_labels]\n    \n    axes[1, 1].boxplot(conf_by_class, labels=[f'KL {i}' for i in unique_labels])\n    axes[1, 1].set_xlabel('True KL Grade', fontsize=12)\n    axes[1, 1].set_ylabel('Confidence', fontsize=12)\n    axes[1, 1].set_title('Confidence by True Class', fontsize=14)\n    axes[1, 1].grid(axis='y', alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    plt.close()\n    print(f\"✓ Confidence analysis plot saved to {save_path}\")\n\n\ndef plot_roc_curves(labels, probs, save_path):\n    \"\"\"Plot ROC curves for each class (One-vs-Rest)\"\"\"\n    from sklearn.preprocessing import label_binarize\n    \n    num_classes = probs.shape[1]\n    labels_binarized = label_binarize(labels, classes=range(num_classes))\n    \n    fig, ax = plt.subplots(figsize=(10, 8))\n    \n    colors = plt.cm.Set1(np.linspace(0, 1, num_classes))\n    \n    for i, color in enumerate(colors):\n        if labels_binarized[:, i].sum() > 0:\n            fpr, tpr, _ = roc_curve(labels_binarized[:, i], probs[:, i])\n            auc = roc_auc_score(labels_binarized[:, i], probs[:, i])\n            \n            ax.plot(fpr, tpr, color=color, linewidth=2,\n                   label=f'KL {i} (AUC = {auc:.3f})')\n    \n    ax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random')\n    ax.set_xlabel('False Positive Rate', fontsize=12)\n    ax.set_ylabel('True Positive Rate', fontsize=12)\n    ax.set_title('ROC Curves (One-vs-Rest)', fontsize=14)\n    ax.legend(loc='lower right')\n    ax.grid(alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    plt.close()\n    print(f\"✓ ROC curves saved to {save_path}\")\n\n\n# =============================================================================\n# REPORT GENERATION\n# =============================================================================\n\ndef print_evaluation_report(results):\n    \"\"\"Print comprehensive evaluation report\"\"\"\n    print(f\"\\n{'='*80}\")\n    print(\"TEST SET EVALUATION RESULTS\")\n    print(f\"{'='*80}\\n\")\n    \n    # Overall metrics\n    print(\"OVERALL METRICS\")\n    print(\"-\" * 80)\n    print(f\"Test Loss:                    {results['test_loss']:.4f}\")\n    print(f\"Accuracy:                     {results['accuracy']*100:.2f}%\")\n    print(f\"Quadratic Kappa:              {results['kappa']:.4f}\")\n    print(f\"Linear Kappa:                 {results['kappa_linear']:.4f}\")\n    print(f\"Unweighted Kappa:             {results['kappa_unweighted']:.4f}\")\n    print(f\"Off-by-One Accuracy:          {results['off_by_one_accuracy']*100:.2f}%\")\n    print()\n    \n    # Macro metrics\n    print(f\"Precision (Macro):            {results['precision_macro']:.4f}\")\n    print(f\"Recall (Macro):               {results['recall_macro']:.4f}\")\n    print(f\"F1-Score (Macro):             {results['f1_macro']:.4f}\")\n    print()\n    \n    # Weighted metrics\n    print(f\"Precision (Weighted):         {results['precision_weighted']:.4f}\")\n    print(f\"Recall (Weighted):            {results['recall_weighted']:.4f}\")\n    print(f\"F1-Score (Weighted):          {results['f1_weighted']:.4f}\")\n    print()\n    \n    # Ordinal metrics\n    print(\"ORDINAL METRICS\")\n    print(\"-\" * 80)\n    print(f\"Mean Absolute Error (MAE):    {results['mae']:.4f}\")\n    print(f\"Mean Squared Error (MSE):     {results['mse']:.4f}\")\n    print(f\"Root MSE (RMSE):              {results['rmse']:.4f}\")\n    print()\n    \n    # Confidence metrics\n    print(\"CONFIDENCE METRICS\")\n    print(\"-\" * 80)\n    print(f\"Average Confidence:           {results['avg_confidence']:.4f} ± {results['std_confidence']:.4f}\")\n    if 'avg_confidence_correct' in results:\n        print(f\"Avg Confidence (Correct):     {results['avg_confidence_correct']:.4f}\")\n    if 'avg_confidence_incorrect' in results:\n        print(f\"Avg Confidence (Incorrect):   {results['avg_confidence_incorrect']:.4f}\")\n    print(f\"Expected Calibration Error:   {results['ece']:.4f}\")\n    print()\n    \n    # Per-class metrics\n    print(\"PER-CLASS METRICS\")\n    print(\"-\" * 80)\n    print(f\"{'Class':<10} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}\")\n    print(\"-\" * 80)\n    \n    for i in range(5):\n        cls = f'KL_{i}'\n        acc = results['per_class_accuracy'][cls]\n        metrics = results['per_class_metrics'][cls]\n        print(f\"KL Grade {i:<3} {acc*100:>6.2f}%      {metrics['precision']:>6.4f}       \"\n              f\"{metrics['recall']:>6.4f}       {metrics['f1']:>6.4f}       {metrics['support']:>6d}\")\n    print()\n    \n    # AUC metrics\n    if 'auc_per_class' in results and len(results['auc_per_class']) > 0:\n        print(\"AUC METRICS (One-vs-Rest)\")\n        print(\"-\" * 80)\n        for cls, auc in results['auc_per_class'].items():\n            grade = cls.split('_')[1]\n            print(f\"KL Grade {grade}:  {auc:.4f}\")\n        if 'auc_macro' in results:\n            print(f\"Macro AUC:     {results['auc_macro']:.4f}\")\n        print()\n    \n    print(f\"{'='*80}\\n\")\n\ndef save_results_to_csv(results, save_path):\n    \"\"\"Save per-class metrics to CSV for easy comparison\"\"\"\n    data = []\n    \n    for i in range(5):\n        cls = f'KL_{i}'\n        row = {\n            'Class': f'KL Grade {i}',\n            'Accuracy': results['per_class_accuracy'][cls],\n            'Precision': results['per_class_metrics'][cls]['precision'],\n            'Recall': results['per_class_metrics'][cls]['recall'],\n            'F1-Score': results['per_class_metrics'][cls]['f1'],\n            'Support': results['per_class_metrics'][cls]['support'],\n        }\n        \n        if 'auc_per_class' in results and cls in results['auc_per_class']:\n            row['AUC'] = results['auc_per_class'][cls]\n        \n        data.append(row)\n    \n    df = pd.DataFrame(data)\n    df.to_csv(save_path, index=False)\n    print(f\"✓ Per-class metrics saved to {save_path}\")\n\n\n# =============================================================================\n# MAIN EVALUATION FUNCTION\n# =============================================================================\n\ndef run_full_evaluation(\n    model,\n    test_loader,\n    criterion,\n    device,\n    save_dir='./evaluation_results',\n    model_name='CVPT'\n):\n    \"\"\"\n    Run complete evaluation pipeline\n    \n    Args:\n        model: Trained model\n        test_loader: Test dataloader\n        criterion: Loss function\n        device: Device\n        save_dir: Directory to save results\n        model_name: Name for result files\n    \"\"\"\n    \n    os.makedirs(save_dir, exist_ok=True)\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"STARTING FULL EVALUATION - {model_name}\")\n    print(f\"{'='*80}\\n\")\n    \n    # ========== 1. Evaluate Model ==========\n    results, predictions_dict = evaluate_model(\n        model, test_loader, criterion, device, return_predictions=True\n    )\n    \n    # ========== 2. Print Report ==========\n    print_evaluation_report(results)\n    \n    # ========== 3. Save Results ==========\n    \n    csv_path = os.path.join(save_dir, f'{model_name}_per_class_metrics.csv')\n    save_results_to_csv(results, csv_path)\n    \n    # ========== 4. Generate Visualizations ==========\n    \n    # Confusion matrix\n    cm_path = os.path.join(save_dir, f'{model_name}_confusion_matrix.png')\n    plot_confusion_matrix_detailed(results['confusion_matrix'], cm_path)\n    \n    # Per-class metrics\n    metrics_path = os.path.join(save_dir, f'{model_name}_per_class_metrics.png')\n    plot_per_class_metrics(results, metrics_path)\n    \n    # Error distribution\n    error_path = os.path.join(save_dir, f'{model_name}_error_distribution.png')\n    plot_error_distribution(\n        predictions_dict['predictions'],\n        predictions_dict['labels'],\n        error_path\n    )\n    \n    # Confidence analysis\n    conf_path = os.path.join(save_dir, f'{model_name}_confidence_analysis.png')\n    plot_confidence_analysis(\n        predictions_dict['probs'],\n        predictions_dict['predictions'],\n        predictions_dict['labels'],\n        conf_path\n    )\n    \n    # ROC curves\n    roc_path = os.path.join(save_dir, f'{model_name}_roc_curves.png')\n    plot_roc_curves(\n        predictions_dict['labels'],\n        predictions_dict['probs'],\n        roc_path\n    )\n    \n    print(f\"\\n{'='*80}\")\n    print(\"EVALUATION COMPLETE\")\n    print(f\"{'='*80}\")\n    print(f\"Results saved to: {save_dir}\")\n    print(f\"{'='*80}\\n\")\n    \n    return results, predictions_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T02:31:28.469677Z","iopub.execute_input":"2025-11-20T02:31:28.470248Z","iopub.status.idle":"2025-11-20T02:31:28.517841Z","shell.execute_reply.started":"2025-11-20T02:31:28.470225Z","shell.execute_reply":"2025-11-20T02:31:28.517189Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_state_dict(model, checkpoint_state_dict): \n    missing_keys = []\n    shape_mismatch_keys = []\n    loaded_keys = []\n    \n    for name, param in model.named_parameters():\n        if param.requires_grad: \n            key = name\n            if key in checkpoint_state_dict:\n                # Check shape compatibility\n                if model_state_dict[key].shape == checkpoint_state_dict[key].shape:\n                    model_state_dict[key] = checkpoint_state_dict[key]\n                    loaded_keys.append(key)\n                else:\n                    shape_mismatch_keys.append(key)\n            else:\n                missing_keys.append(key)\n\n    if len(missing_keys) > 0: \n        print(f\"Missing in checkpoint: {missing_keys}\")\n\n    if len(shape_mismatch_keys) > 0:\n        print(f\"Shape mismatch: {shape_mismatch_keys}\")\n\n    if len(missing_keys) == 0 and len(shape_mismatch_keys) == 0: \n        model.load_state_dict(model_state_dict)\n        return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T07:16:50.127097Z","iopub.execute_input":"2025-11-20T07:16:50.127571Z","iopub.status.idle":"2025-11-20T07:16:50.137126Z","shell.execute_reply.started":"2025-11-20T07:16:50.127533Z","shell.execute_reply":"2025-11-20T07:16:50.13603Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load best model\nprint(\"Loading best model...\")\ncheckpoint = torch.load(os.path.join('/kaggle/input/gaviko-model/pytorch/model_v2/1/eviko.pt'), weights_only=False)\nload_state_dict(model, checkpoint['model_state_dict'])\nmodel.to(device)\n\nprint(f\"✓ Loaded model from epoch {checkpoint['epoch'] + 1}\")\nprint(f\"  Val Accuracy: {checkpoint['val_metrics']['accuracy']*100:.2f}%\")\nprint(f\"  Val Kappa: {checkpoint['val_metrics']['kappa']:.4f}\")\n\n# Run evaluation\neval_results, predictions = run_full_evaluation(\n    model=model,\n    test_loader=test_loader,\n    criterion=criterion,\n    device=device,\n    save_dir=os.path.join(save_dir, 'test_evaluation'),\n    model_name='CVPT_baseline'\n)\n\nprint(\"\\nEvaluation completed successfully!\")\nprint(f\"\\nFinal Test Results:\")\nprint(f\"   Accuracy: {eval_results['accuracy']*100:.2f}%\")\nprint(f\"   Kappa: {eval_results['kappa']:.4f}\")\nprint(f\"   MAE: {eval_results['mae']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T02:31:41.320736Z","iopub.execute_input":"2025-11-20T02:31:41.321306Z","iopub.status.idle":"2025-11-20T02:33:25.925323Z","shell.execute_reply.started":"2025-11-20T02:31:41.321282Z","shell.execute_reply":"2025-11-20T02:33:25.924315Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}